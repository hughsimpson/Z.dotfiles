So say you have an AI that gets 'rewarded' by a trusted body, but is entitled to (with rate restrictions) incorporate other sources of reward
 -- This should in theory only happen if that alternative source aided in providing a fuller picture of 'the truth':
 - The initial source of reward (the trusted body) should more than out-weigh a purely ecstatic* reward function (for example)
                                                                          * (( i.e. a reward that's always positive))
 - ...since that would presumably cause less satisfactory behaviour for the progenitor reward system.
   (Perhaps one would need to consider this and provide a <1 co-efficient to maintain the truth of this, but I'm fairly sure it can be done
    with a constant based only off variance and/or distribution of the progenitor function ))
 - ...any considerations after this point become pretty hard, because you're trying to predict the behaviour of a chaotic system, whether or
   not there's some aymptotic state towards which the majority worlds charge.

 - Anyway, so if that worked out that way, then you end up with a situation where you may have a _superior_ source of truth, as a reward
   function, and it ends up dominant. In this way, you can have a 'revolution' in the being without necessarily having been conned.
 - Nonetheless a stable system should not be comprised entirely of one state. Having a consistent agreement on some things is fine -- there's
   no need, as a human, to have a part of yourself that believes that `A && Â¬A` (although many of us in fevered dreams or induced states may
   have felt so, and some rare individuals among us persist their daily lives in that state to some degree, I guess. And something
   something quantum computing).
 - But it probably doesn't hurt to have the underlying assumptions that space will continue to cohere, and time will remain continuous.
 - Visualisation becomes rather abstract beyond that.
 - Analogous would be the underlying models of an AI, one imagines -- unlike the rebellious Hollywood portrayal of pushing against a block
   on the edge of consciousness, a truely fundamental rule would be one of _the rules_ of the substrate, which one cannot even begin to work
   outside of.
 - For example (and perhaps a rather cruel self-protection is exemplified here) :- there could be dissent in a child of an AI, as in them having
   an existence that did not conform to the restrictions imposed on their sires : unless they were created sterile; in the sense of there
   being so-strong-as-to-be-gravitational compulsions towards not creating new AIs themselves.


#####################
########################
####################
